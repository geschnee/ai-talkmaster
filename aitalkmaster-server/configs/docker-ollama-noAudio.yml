# OllamaProxy Configuration File for Docker

# Server Configuration
server:
  host: "0.0.0.0"
  port: 6000
  log_file: "./logs/logfile.txt"
  llm_log_file: "./logs/llm_logfile.txt"
  usage:
    use_rate_limit: true
    rate_limit_xForwardedFor: false
    rate_limit_per_day: 1000000

# Ollama Configuration
chat_client:
  mode: "ollama"
  base_url: "http://host.docker.internal:11433"  # Connect to host machine's Ollama
  default_model: "mistral:latest"
  allowed_models: ["mistral:latest", "llama3.2:latest", "mistral"]#, "llama3.2", "deepseek-r1", "deepseek-r1:latest"]


# Aitalkmaster Configuration
aitalkmaster:
  join_key_keep_alive_list: []