# OllamaProxy Configuration File for Docker

# Server Configuration
server:
  host: "0.0.0.0"
  port: 7999
  log_file: "./logs/logfile.txt"

# Ollama Configuration
chat_client:
  mode: "ollama"
  base_url: "http://host.docker.internal:11434"  # Connect to host machine's Ollama
  default_model: "mistral:latest"
  allowed_models: ["mistral:latest", "llama3.2:latest", "mistral", "llama3.2"]




# Aitalkmaster Configuration
aitalkmaster:
  join_key_keep_alive_list: []