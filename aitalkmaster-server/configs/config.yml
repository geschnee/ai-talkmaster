# OllamaProxy Configuration File

# Server Configuration
server:
  host: "0.0.0.0"
  port: 7999

# Ollama Configuration
chat_client:
  mode: "openai" # "openai" or "ollama"
  key_file: "openai_key.txt"
  base_url: "http://localhost:11434"
  # valid_models: ["llama3.2", "mistral", "deepseek-r1"]
  valid_models: ["gpt-4o-mini", "gpt-4o", ]

# TODO make the chat client URL be used everywhere (also when the mode is openai (meaning another provider should be usable))

# use host.docker.internal for the base url if the Ollama or Kokoro server is running on the host machine

# OpenAI Configuration
audio_client:
  mode: "openai" # "openai" or "kokoro"
  key_file: "openai_key.txt"
  base_url: "http://localhost:8880/v1"  # is used when mode == "kokoro" -> Kokoro audio server
  default_voice: "alloy"
  default_model: "tts-1"
  valid_voices: ["alloy", "ash", "ballad", "coral", "echo", "fable", "onyx", "nova", "sage", "shimmer", "verse"]
  valid_models: ["tts-1", "tts-1-hd", "gpt-4o-mini-tts", "kokoro"]


liquidsoap_client:
  host: "liquidsoap"
  port: 1234

icecast_client:
  host: "icecast"
  port: 8000
  admin_password: "password"

# Aitalkmaster Configuration
aitalkmaster:
  join_key_keep_alive_list: []
  log_file: "./logs/logfile.txt"
